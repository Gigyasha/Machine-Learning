{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE FOR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is a non parametric or supervised machine learning algorithm. It can be used for classifcation or regression purposes<br>\n",
    "__Decision Tree for Regressison__ :- When decision tree is used for predicting continous values or when the target values are continious.<br>\n",
    "__Decision Tree for Classification__ :- When decision Tree is used for predicting classes or when the target values are discrete  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 <u>EXPALINATION PART</u>\n",
    "lets understand the basis termology .<br>\n",
    "rootnode is the starting point which includes the complete set of training data.<br>\n",
    "subnodes are the nodes which has incong arrow as well as outgoing arrow it does not contains complete data.<br>\n",
    "leaf node is the node which only has incoming arrow and no outgoing arrows to it.<br>\n",
    "\n",
    "\n",
    "__Process__\n",
    "Decision tree is just like binary search tree so to create a tree we need to divide the data and for dividing or splitting  the data we need to choose some features and some cut point value eg i have choosen a feacture x and a particular value of x so on the basis of these two values we split our data the same process goes on for each and every node.\n",
    "lets understand this algorithm by talking an example.\n",
    "In the root node we have the entire training data and in the left nod\n",
    "Lets see we have a dataset having 2 features X and A where X is the marks of students and A represents their attendance and the target variable is Y their salary . We have to create such a decision tree which can help us in predicting the salary values on an unseen data using the training data.<br>\n",
    "so we have to create a tree which predicts the salaray values\n",
    "<img src=\"decisiontree1.png\" width=\"500\" height=\"100\"/>\n",
    "#### To Create a Tree\n",
    "At the root node we have the entire training data and to split that say we have choosen  X feature and x50 as out cutpoint value\n",
    "so on the basis of these two values we diviide out data into two nodes left node and right node.now similary on the left node we select attendace feature and a cutpoint value of attendance to divide out data .and similarly on the right node we select a featuure and cut point value of that feature the same process took place for every node so that we can split the data .<br>\n",
    "<img src=\"decisiontree2.png\" width=\"500\" height=\"100\"/>\n",
    "#### On testing data\n",
    "when we have a testing example and we when we apply decision tree algorithm so according to the descision tree cut point vaues it reaches to a leaf node but the leaf node might contains a lot of training example so what the ans or predicted value will be for that testing example \n",
    "so the predicted salary for that testing example will be the average of all the target values presented in that particular leaf node.<br>\n",
    "### But the main Question is how should we select what feature to choose and what cut point to choose to create the best decision tree.\n",
    "These are the two main criteria for creating a descision tree which is best fitted for the the data.<br>\n",
    "lets start with root node and see how to choose which is the best feature and the cut point .Before splitting the root node or any node , what we do is for every feature lets take an example of feature X , and it contains 100 examples or 100 feature values so firstly at every xi value or at every x example from x1 to x100 we assume  it as cut point any split the node (just to see that point is good or not)  after splitting we calculated the mean square error ie actual - predicted(average of all the target value presented in that node)\n",
    "$$ \\text{Predicted Target} \\;\\;\\; \\bar{Y_{L}} = \\frac{y1+y2+....+y_{n_{L}}}{n_{L}}  $$\n",
    "$$ y1,y2.....y_{n_{L}} \\text{is actual target values}$$\n",
    "so Mean Square error will be,\n",
    "$$ MSE_{\\text{for left node}} = \\frac{(y1-\\bar{Y})^{2}+(y2-\\bar{Y})^{2}+......(y_{n_{L}}-\\bar{Y})^{2}}{n_{L}}$$\n",
    "same for right node and we take thier weigheted average.\n",
    "$$ \\text{Weighted Average} = \\frac{n_{L}MSE_{L}+n_{R}MSE_{R}}{n_{L}+n_{R}}$$\n",
    "and for x1 to x100 we will  choose xut point and calculate weighted average and till we get a minimum weighted Average value so if we have 5 feature we get 5 Weigheted Average values.<br>\n",
    "now the feature which has the least Weighted Average value is our chooosen feature and its cut point value will be our choosen cut point value for that node<br>\n",
    "We will do the same process for every node this process is recursive in nature and called __Recursive Binary Splitting__.<br>\n",
    "### Disadvantage of Decision Tree.\n",
    "When we grow our tree fully without early stopping such that each of the leaf node conatins only one value left/ex and on the same decision tree we get predict the value of training data then the accuracy will come out as 100% because each training example with reach to matching leaf node and predict the same and this causes 100% accuracy on trainig data but on the other hand such cases gives very bad accuracy on testing data and this is called __Overfitting__.\n",
    "Decision Tree if grown or extended too long such that the leaf node conatins single example then this rise to Overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
