{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE FOR CLASSIFICATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is a non parametric or supervised machine learning algorithm. It can be used for classifcation or regression purposes<br>\n",
    "__Decision Tree for Regressison__ :- When decision tree is used for predicting continous values or when the target values are continious.<br>\n",
    "__Decision Tree for Classification__ :- When decision Tree is used for predicting classes or when the target values are discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 <u>EXPALINATION PART</u>\n",
    "lets understand the basis termology .<br>\n",
    "rootnode is the starting point which includes the complete set of training data.<br>\n",
    "subnodes are the nodes which has incong arrow as well as outgoing arrow it does not contains complete data.<br>\n",
    "leaf node is the node which only has incoming arrow and no outgoing arrows to it.<br>\n",
    "\n",
    "__PROCESS__<br>\n",
    "Decision Tree for classification is similar like regression but the way of calculation are bit different lets understand the process for Classification scenerio.\n",
    "To create a decision tree we need to split our data but on what grounds or on what fcators we split our data.\n",
    "for splitting a data at every node we make some calculations which helps us in determining which factor to choose for splitting and what cut point value of that factor.so what calculations we exactly do .In case of regression we calculate mean square error to calculate an average stnd deviation from original data. But in case of classification we find Entropy not MSE because we deal with classes.\n",
    "<br>\n",
    "__To Create a Trees__\n",
    "Lets understand the process by taking root node and we need to find the splitiing conditions/parameters . lets take the feature X , for every value of x we find something called entropy(Self entropy) which is also called randomness . if we have only one type of label then our entropy is less . for example we take x50 as our cut point value which divides the data into 2 parts one for left node and other for right and left node contains more than one labels so which label to assign . to assign a label we calculate self entropy<br>\n",
    "But why not Cross Entropy?<br>\n",
    "because to calculate cross entropy we use sigmoid function and sigmoid function conatins theta's which need to be calculated and that makes our algorithm parametric that's why we calculate self entropy.<br>\n",
    "This Is the Formula For Self Entropy\n",
    "$$ E =  -\\sum_{K=0}^{1} Q_{L}(c^{i}=K)log_{e}Q_{L}(c^{i}=K) \\ \\text{   Where 0-1 are the classes}$$\n",
    "$$ E =  -[ Q_{L}(c^{i}=0)log_{e}Q_{L}(c^{i}=0)+ Q_{L}(c^{i}=1)log_{e}Q_{L}(c^{i}=1)]$$<br>\n",
    "When We calculate it for a node we do it for all the examples present in that node<br>\n",
    "\n",
    "for left node the Average Self entropy is,\n",
    "$$ E_{L} =  - \\sum_{i=1}^{n_{L}} \\sum_{K=0}^{1} \\frac{Q_{L}(c^{i}=K)log_{e}Q_{L}(c^{i}=K)}{n_{L}} \\ \\text{  nL is no of ex in left node}$$<br>\n",
    "similarly Average Self entropy for Right Node\n",
    "$$ E_{R} =  - \\sum_{i=1}^{n_{R}} \\sum_{K=0}^{1} \\frac{Q_{L}(c^{i}=K)log_{e}Q_{L}(c^{i}=K)}{n_{R}} \\ \\text{  nR is no of ex in Right node}$$<br>\n",
    "The Weighted entropy or Metric for Node Purity is<br>\n",
    "$$Node Purity = \\frac{n_{L}E_{L}+n_{R}E_{R}}{n_{L}+n_{R}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want that average Weighted self entropy for a node shoule be minimum and increase the node purity.\n",
    "The Ideal case should be when each node conatins only one label it means there should'nt be any contaiminaton<br>\n",
    "At every node for each example of each feature we calculate weighted Average entropy and the value at which it is minimum we choose that as cut point value and that feature will be the choosen feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tasks We are going to do will be\n",
    "1. Prepare and Load the data\n",
    "2. Divide the data into training data and testing data\n",
    "3. Helper Functions\n",
    "    3.2. BASE CONDITION---First we will check the node Purity it means if the examples present in the node only contains    single unique label and the other base condition for pruning the tree will be maximum depth if the tree will reach maxium depth then also we will stop their and peform classification.<br>\n",
    "    3.3. IF the node is not pure or it is not at pruning level so we need to split the node but on the basis of what parameters we should split it for that.\n",
    "       3.3.a Find POTENTIAL SPLITS.\n",
    "       3.3.b Among the potential splits DETERMINE THE BEST SPLIT.\n",
    "       3.3.c SPLIT the data.\n",
    "       3.3.d To determine the best split take the splitted data and calculate OVERLL ENTROPY.\n",
    "       3.3.e To calculate overall entropy first calculate ENTROPY.\n",
    "    3.4 A function named as DECISION_TREE_ALGORITHM is used to implement these function.<br>\n",
    "    3.5 Now Preditions are made on testing data using accuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
